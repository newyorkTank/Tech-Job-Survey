#Importsfrom collections import Counterfrom bs4 import BeautifulSoup as bs4import pandas as pdimport requestsimport sysdef remove_duplicates(values):    output = []    seen = set()    for value in values:        # If value has not been encountered yet,        # ... add it to both list and set.        if value not in seen:            output.append(value)            seen.add(value)    return outputdef sorted_dupless(lineas):    words = []    for line in lineas:  # Cleanups        # word = text.split('\n')  # Use first line to test parsing        line = line.strip()        if len(line) > 1:            words.append(line.lower())    words = remove_duplicates(words)    words.sort()    return words# function inListdef inList(word, list):    if word in list:        return True    else:        return False#################   Begin###### var declarationslist_results = []list4counts = []#How deep to search given parametersdepth = 20  #Number of job ads per citymin = int(depth/6)  #Threshold for caring####Different sets of categoriesall_job_cats = {'web':'web-html-info-design','sof':'software-qa-dba-etc','tch':'technical-support','sad':'systems-networking'}code_job_cats = {'web':'web-html-info-design','sof':'software-qa-dba-etc'}tech_sup = {'tch':'technical-support'}job_cats = code_job_cats####What cities to searchone_city = ['losangeles']livable_cities = ['portland','houston','miami','miami','boston','washingtondc','sfbay','denver','sandiego','tampa','minneapolis','boise','raleigh','coloradosprings']use_text_file = []cities = use_text_fileif(len(cities) == 0):       # If no cities declared above, use the comprehensive text file.    cities_file = 'cities.txt'    cw = open(cities_file, 'r', encoding='utf-8', errors='ignore')    clines = cw.readlines()    cw.close()    for line in clines:  # Clean off newline        line = line.strip()        cities.append(line.lower())# DeDupe and sort the Listcities = sorted_dupless(remove_duplicates(cities))#### Which set of words are we looking for?enterprise_mgmt = ['barracuda','nagios','kaseya','vsa','prtg','solarwinds','opmanager','whatsup','zabbix','incinga','datadog','connectwise']web_frameworks = ['flask','django','asp.net','coldfusion','elixir','haskell','rails','sinatra','zope','pyramid','cherrypy','web2py','netty','brutos','vaadin','spring','tornado','uliweb','google app engine','meteor','openui5','vue.js','node.js','cake','codeigniter','drupal','symfony','fuelphp','laravel','joomla','angular','ember.js','bootstrap','react','node.js','angular','.net','spring','cordova','tensorflow','xamarin','spark','hadoop','torch']web_design = ['wix','squarespace','weebly','sketch','kompozer','macaw','ezgenerator','amaya','freeway','expression web','artisteer','xara','openelement','pinegrow','fusion','google web designer','themler','mobirise','wordpress','dreamweaver','balsamiq','mockplus','muse','toaster','coffeecup']devops = ['aws','azure','hyperv','hyper-v','docker','rocket','unik','ansible','puppet','chef','jenkins','bamboo','selenium','cucumber','apache jmeter','kubernetes','swarm','mesos','elastic beanstalk','octopus','vamp','newrelic','kibna','datalog','vmware','esx']big_data = ['mongo','hadoop','data lake','bigpanda']net_equipo = ['switch','cisco','ibm','firewall','blade server','san storage','kvm']languages = ['erlang','scala','ocaml','clojure','groovy','perl','javascript','sql','bash','python','php','typescript','ruby','swift','assembly','go','objective-c','matlab','kotlin','grooby','scala','julia','clojure','rust','c#','c++']use_text_file = []gwords = web_frameworksif(len(gwords) == 0):      # If no list declared above, use the comprehensive text file.    gwords_file = 'search_words.txt'    gw = open(gwords_file, 'r', encoding='utf-8', errors='ignore')    glines = gw.readlines()    gw.close()    for line in glines:  # Clean off newline        line = line.strip()        gwords.append(line.lower())# DeDupe and sort the Listgwords = sorted_dupless(remove_duplicates(gwords))################################################## ######## Now begin crawling pageschar_kount = 1 # Used to break line in displaymax_ck = 15Ads = 1  # for count of ads scrappedprint('Prepared ' + str(len(cities)) + ' markets, ' + str(len(job_cats)) + ' job categories, to a depth of ' + str(depth))print('Begin Scraping (dots are categories per market)')for slum in cities:    if(char_kount + len(slum) > max_ck):        char_kount = 0        print('\n' + slum, end='',flush=True)    else:        print(slum, end='', flush=True)    char_kount += len(slum)    for cat in job_cats:        url = 'https://' + slum + '.craigslist.org/d/' + job_cats[cat] + '/search/' + cat        try:            rsp_page = requests.get(url, params='')            if(rsp_page.status_code != 200):                print("STATUS_CODE: " + rsp_page.status_code + " for URL: " + url, end='', flush=True)            else:                ####  NEED A CHECK FOR RESPONSE 200                search_html = bs4(rsp_page.text, 'html.parser')                jobs = search_html.find_all('p', attrs={'class': 'result-info'})                deep = depth if depth <= len(jobs) else len(jobs)                for x in range(0, deep):                    search_element = jobs[x]                    dater = search_element.findAll(attrs={'class': 'result-date'})[0].text                    title = search_element.findAll(attrs={'class': 'result-title hdrlnk'})[0].text                    link = search_element.parent.contents[1].attrs['href']                    try:                        rsp_sub = requests.get(link, params='')                        if (rsp_sub.status_code != 200):                            print("STATUS_CODE: " + rsp_page.status_code + " for URL: " + url, end='', flush=True)                        else:                            job_html = bs4(rsp_sub.text, 'html.parser')                            job = job_html.find_all('section', attrs={'id': 'postingbody'})                            d = job[0].text                            # would like to pull email from here, also location                            d = d.lower()                            for p in gwords:                                if inList(p, d):                                    dict_results = {}                                    list4counts.append(p)                                    dict_results['term'] = p                                    dict_results['city'] = slum                                    dict_results['category'] = cat                                    dict_results['date'] = dater                                    dict_results['title'] = title                                    dict_results['link'] = link                                    dict_results['snippet'] = d[(d.index(p) - 30):(d.index(p) + 20)]                                    list_results.append(dict_results)                            Ads += 1                            char_kount += 1                            if Ads % 5 == 0:                                print(Ads, end='', flush=True)                            else:                                print('.', end='', flush=True)                    except Exception as e:                        print(e, end='', flush=True)                        continue        except Exception as e:            print(e, end='', flush=True)            continue#Show all entries for one search word, to compare two technologies directlys_word = 'vsa'ss_word = 'kaseya'#print(next((item for item in list_results if item['term'] == s_word)))list_dict_4sword = [i for i in list_results if i['term'] == s_word]for v in zip(list_dict_4sword):    print(v)#input('Press <ENTER> to continue')# Sort and print quantities of found wordsprint('')print('Total Ads scraped:' + str(Ads))print('Here are the results:')sortedWords = Counter(list4counts)s = [(k, sortedWords[k]) for k in sorted(sortedWords, key=sortedWords.get, reverse=True)]for k,v in s:    if v > min - 1:        print(str(v) + ', ' + k)